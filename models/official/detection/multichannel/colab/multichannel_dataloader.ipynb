{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RgqiYb1pjrmu"
      },
      "source": [
        "To run this notebook:\n",
        "\u003e g4d your_citc_client;\u003cbr\u003e\n",
        "\u003e /google/data/ro/teams/colab/tensorflow --port=8888\u003cbr\u003e\n",
        "\u003e Connect to port 8888 in the top right of this notebook.\u003cbr\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DEqZnn6vX9bh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "from google3.pyglib import flags\n",
        "import numpy as np\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from colabtools import adhoc_import\n",
        "\n",
        "with adhoc_import.Google3CitcClient(\"multichannel_training\", username=\"salbro\", behavior=\"preferred\", verbose=True):\n",
        "  from dataloader import input_reader\n",
        "  from dataloader import mode_keys as ModeKeys\n",
        "  from hyperparameters import params_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KErZj5y05dEv"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYCqUIZbmfHb"
      },
      "source": [
        "## Set these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nwCWyKoY6S3-"
      },
      "outputs": [],
      "source": [
        "extra_channel_key = \"image/relative_altitude\"\n",
        "extra_channel_mean = 103\n",
        "extra_channel_std = 76\n",
        "\n",
        "_TRAIN_FILE_NAME = '/cns/ue-d/home/geo-imagery-lerna-models-dev/geo-sky-image-processing/mpallone/training_sets/eos/multichannel/geoalign.test.normalized.tfrecord*'\n",
        "output_size = [1024, 1024]\n",
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OQ8i_gwemg4L"
      },
      "source": [
        "## Set the other params."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fo7ICj-xNI7b"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "anchor_size = 4.0\n",
        "\n",
        "min_level = 3\n",
        "max_level = 7\n",
        "num_scales = 3\n",
        "aspect_ratios = [1.0, 2.0, 0.5]\n",
        "max_num_instances = 100\n",
        "skip_crowd_during_training = True\n",
        "use_autoaugment=False\n",
        "\n",
        "SHAPEMASK_RESNET_FROZEN_VAR_PREFIX = r'(resnet\\d+/)conv2d(|_([1-9]|10))\\/'\n",
        "params = params_dict.ParamsDict({\n",
        "    'type': 'shapemask',\n",
        "    'train': {\n",
        "        'total_steps': 45000,\n",
        "        'learning_rate': {\n",
        "            'learning_rate_steps': [30000, 40000],\n",
        "        },\n",
        "        'frozen_variable_prefix': SHAPEMASK_RESNET_FROZEN_VAR_PREFIX,\n",
        "    },\n",
        "    'eval': {\n",
        "        'type': 'shapemask_box_and_mask',\n",
        "        'mask_eval_class': 'all',  # 'all', 'voc', or 'nonvoc'.\n",
        "    },\n",
        "    'architecture': {\n",
        "        'parser': 'shapemask_parser',\n",
        "        'backbone': 'resnet',\n",
        "        'multilevel_features': 'fpn',\n",
        "        'use_bfloat16': True,\n",
        "    },\n",
        "    'anchor': {\n",
        "      'min_level': min_level,\n",
        "      'max_level': max_level,\n",
        "      'num_scales': num_scales,\n",
        "      'aspect_ratios': aspect_ratios,\n",
        "      'anchor_size': anchor_size,\n",
        "    },\n",
        "    'shapemask_parser': {\n",
        "        'output_size': [1024, 1024],\n",
        "        'match_threshold': 0.5,\n",
        "        'unmatched_threshold': 0.5,\n",
        "        'aug_rand_hflip': True,\n",
        "        'aug_scale_min': 0.8,\n",
        "        'aug_scale_max': 1.2,\n",
        "        'skip_crowd_during_training': True,\n",
        "        'max_num_instances': 100,\n",
        "        'use_bfloat16': True,\n",
        "        # Shapemask specific parameters\n",
        "        'mask_train_class': 'all',  # 'all', 'voc', or 'nonvoc'.\n",
        "        'use_category': True,\n",
        "        'outer_box_scale': 1.25,\n",
        "        'num_sampled_masks': 8,\n",
        "        'mask_crop_size': 32,\n",
        "        'mask_min_level': 3,\n",
        "        'mask_max_level': 5,\n",
        "        'box_jitter_scale': 0.025,\n",
        "        'upsample_factor': 4,\n",
        "        'extra_channel_keys': [extra_channel_key]\n",
        "    },\n",
        "    'retinanet_head': {\n",
        "        'min_level': 3,\n",
        "        'max_level': 7,\n",
        "        # Note that `num_classes` is the total number of classes including\n",
        "        # one background classes whose index is 0.\n",
        "        'num_classes': 2,\n",
        "        'anchors_per_location': 9,\n",
        "        'retinanet_head_num_convs': 4,\n",
        "        'retinanet_head_num_filters': 256,\n",
        "        'use_separable_conv': False,\n",
        "        'use_batch_norm': True,\n",
        "        'batch_norm': {\n",
        "            'batch_norm_momentum': 0.997,\n",
        "            'batch_norm_epsilon': 1e-4,\n",
        "            'batch_norm_trainable': True,\n",
        "            'use_sync_bn': False,\n",
        "        },\n",
        "    },\n",
        "    'shapemask_head': {\n",
        "        'num_classes': 2,\n",
        "        'num_downsample_channels': 128,\n",
        "        'mask_crop_size': 32,\n",
        "        'use_category_for_mask': False,\n",
        "        'num_convs': 4,\n",
        "        'upsample_factor': 4,\n",
        "        'shape_prior_path': '',\n",
        "        'batch_norm': {\n",
        "            'batch_norm_momentum': 0.997,\n",
        "            'batch_norm_epsilon': 1e-4,\n",
        "            'batch_norm_trainable': True,\n",
        "            'use_sync_bn': False,\n",
        "        },\n",
        "    },\n",
        "    'retinanet_loss': {\n",
        "        'num_classes': 2,\n",
        "        'focal_loss_alpha': 0.4,\n",
        "        'focal_loss_gamma': 1.5,\n",
        "        'huber_loss_delta': 0.15,\n",
        "        'box_loss_weight': 50,\n",
        "    },\n",
        "    'shapemask_loss': {\n",
        "        'shape_prior_loss_weight': 0.1,\n",
        "        'coarse_mask_loss_weight': 1.0,\n",
        "        'fine_mask_loss_weight': 1.0,\n",
        "    },\n",
        "    'postprocess': {\n",
        "        'min_level': 3,\n",
        "        'max_level': 7,\n",
        "    },\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ldMtWWRKmx-7"
      },
      "source": [
        "# Run a session, getting images and labels from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nFf20C_3YKRh"
      },
      "outputs": [],
      "source": [
        "sess = tf.Session()\n",
        "with sess.as_default():\n",
        "  input_fn = input_reader.InputFn(_TRAIN_FILE_NAME, params, ModeKeys.TRAIN)\n",
        "  dataset = input_fn({'batch_size': batch_size})\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
        "  images, labels = iterator.get_next()\n",
        "  images, labels = sess.run([images, labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aHXj2oZXm0dp"
      },
      "source": [
        "# Show in Matplotlib as a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cnzAhra3kHrb"
      },
      "outputs": [],
      "source": [
        "colormap = (np.random.rand(num_classes+2,3) + 0.3) / 1.3\n",
        "for i, im in enumerate(images):\n",
        "  plt.figure(figsize=[10,10])\n",
        "\n",
        "  rgb = im[:,:,:3] * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
        "  plt.imshow(rgb);\n",
        "\n",
        "  extra_channels = im[:,:,3:]*extra_channel_std + extra_channel_mean\n",
        "  extra_channels = np.concatenate((extra_channels, extra_channels, extra_channels), axis=2)\n",
        "  plt.imshow(extra_channels.astype(int))\n",
        "\n",
        "  ax = plt.gca()\n",
        "  for level in range(min_level, max_level + 1):\n",
        "    locations = np.where(labels['cls_targets'][level][i] \u003e= 0)\n",
        "    batch, h, w, num_anchors_per_location = labels['cls_targets'].shape\n",
        "    anchor_boxes = labels['anchor_boxes'][level].reshape(batch, h, w, num_anchors_per_location, 4)\n",
        "    if len(locations[0]) \u003e 0:\n",
        "      for y, x, anchor_index in zip(*locations):\n",
        "        cls = labels['cls_targets'][level][i, y, x, anchor_index]\n",
        "        # Draws the anchor.\n",
        "        anchor = anchor_boxes[i, y, x, anchor_index]\n",
        "        wa = anchor[3] - anchor[1]\n",
        "        ha = anchor[2] - anchor[0]\n",
        "        ycenter_a = anchor[0] + 0.5 * ha\n",
        "        xcenter_a = anchor[1] + 0.5 * wa\n",
        "        lower_left = (anchor[1], anchor[0])\n",
        "        rect = patches.Rectangle(lower_left, wa, ha,linewidth=1,edgecolor=colormap[cls,:],facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # Draws ground truth box assigned to the anchor.\n",
        "        ty, tx, th, tw = labels['box_targets'][level][i, y, x, anchor_index*4:(anchor_index+1)*4]\n",
        "        w = np.exp(tw) * wa\n",
        "        h = np.exp(th) * ha\n",
        "        ycenter = ty * ha + ycenter_a\n",
        "        xcenter = tx * wa + xcenter_a\n",
        "        ymin = ycenter - h / 2.\n",
        "        xmin = xcenter - w / 2.\n",
        "        rect = patches.Rectangle((xmin, ymin), w, h, linewidth=3, linestyle='-.', edgecolor=colormap[cls+1,:],facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "multichannel_dataloader.ipynb",
      "provenance": [
        {
          "file_id": "/piper/depot/google3/platforms/deepsea/ffds/models/mask_rcnn/colab/dataloader.ipynb",
          "timestamp": 1540598664173
        },
        {
          "file_id": "/piper/depot/google3/robotics/perception/data/dataloader/visualize_dataloader.ipynb?workspaceId=tsungyi:tpu_detector:22:citc",
          "timestamp": 1513714057869
        },
        {
          "file_id": "1yhzTK_POCcxrWQQRhIzlScFcBkZMXrRU",
          "timestamp": 1513713974109
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
